# LLM-Evaluator

<div align="center">

**[English](#english) | [ä¸­æ–‡](#chinese)**

A lightweight, flexible LLM evaluation framework for academic research and benchmarking.

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

</div>

---

<a name="english"></a>
## ğŸ‡¬ğŸ‡§ English

### ğŸ“– Overview

LLM-Evaluator is a modular evaluation pipeline designed for testing Large Language Models on custom datasets. It supports multiple answer formats (LaTeX, GSM8K, natural language) and provides an extensible framework for academic evaluation tasks.

### âœ¨ Features

- ğŸ¯ **Multi-format Answer Extraction**: Supports `\boxed{}`, GSM8K format, and natural language patterns
- ğŸ”„ **Automatic Retry Mechanism**: Handles API rate limits and network issues gracefully
- ğŸ“Š **JSONL Dataset Format**: Easy to create and maintain test cases
- ğŸŒ **Hugging Face Router Support**: Compatible with various LLM providers
- âš™ï¸ **Environment-based Configuration**: Secure API key management via `.env`
- ğŸ§ª **Reproducible Results**: Zero temperature for deterministic evaluation
- ğŸ“ˆ **Interactive Dashboard**: Streamlit interface for result visualization and analysis

### ğŸš€ Quick Start

#### 1. Installation

```bash
# Clone the repository
git clone <your-repo-url>
cd LLM-Evaluator-1

# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

#### 2. Configuration

Create a `.env` file in the project root:

```env
HF_TOKEN=your_huggingface_token_here
HF_API_BASE=https://router.huggingface.co/v1
DEFAULT_MODEL=deepseek-ai/DeepSeek-V3.2:novita
```

#### 3. Prepare Test Data

Create your test cases in `data/test_cases.jsonl`:

```json
{"id": 1, "question": "If x + 5 = 12, what is x?", "answer": "7"}
{"id": 2, "question": "What is 15 multiplied by 3?", "answer": "45"}
```

#### 4. Run Evaluation

```bash
# Using the shell script (recommended)
./run_eval.sh

# Or directly with Python
python main.py --data_path data/test_cases.jsonl
```

#### 5. Data Visualization (Optional)

Generate visual charts to analyze results:

```bash
# Display charts interactively
python main.py --visualize

# Save charts to file
python main.py --save-viz evaluation_results.png
```

**Example Output:**

![Visualization Example](/Users/papersiii/.gemini/antigravity/brain/c979daa4-a75f-470d-8e3f-9993c807669d/evaluation_results.png)

- **Pie Chart**: Overall accuracy distribution (correct vs incorrect)
- **Bar Chart**: Per-question results (green = correct, red = incorrect)

#### 6. Interactive Dashboard

Launch the web interface for an easier evaluation experience:

```bash
streamlit run app.py
```

**Features:**
- ğŸ“ **File Upload**: Drag and drop your JSONL datasets
- âš¡ **Async Evaluation**: Concurrent processing with rate limiting
- ğŸ•¸ï¸ **Radar Chart**: Visual analysis of model performance across categories
- ğŸ“Š **Detailed Metrics**: View accuracy trends and specific failure cases

### ğŸ“ Project Structure

```
LLM-Evaluator-1/
â”œâ”€â”€ main.py                 # Main evaluation script
â”œâ”€â”€ run_eval.sh            # Execution wrapper with auto-detection
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ .env                   # Environment configuration (create this)
â”œâ”€â”€ data/
â”‚   â””â”€â”€ test_cases.jsonl  # Test dataset
â””â”€â”€ src/
    â”œâ”€â”€ model_client.py   # LLM API client
    â””â”€â”€ scorer.py         # Answer extraction and scoring logic
```

### ğŸ”§ Advanced Usage

#### Custom Data Path

```bash
./run_eval.sh path/to/custom_data.jsonl
```

#### Programmatic Usage

```python
from src.model_client import LLMClient
from src.scorer import exact_match_scorer

client = LLMClient()
response = client.get_response("What is 2 + 2?")
is_correct = exact_match_scorer(response, "4")
```

### ğŸ“Š Supported Answer Formats

The scorer automatically extracts answers from:

- **LaTeX**: `\boxed{42}`
- **GSM8K**: `#### 42`
- **Natural Language**: `"The answer is: 42"`
- **Chinese**: `"ç­”æ¡ˆæ˜¯ï¼š42"`
- **Plain Numbers**: Last number in the response

### ğŸ› ï¸ Customization

#### Adding New Scoring Patterns

Edit `src/scorer.py` and add patterns to the `patterns` list:

```python
patterns = [
    r'\\boxed\{([^}]+)\}',
    r'your_custom_pattern_here',
    # ...
]
```

#### Using Different Models

Update `DEFAULT_MODEL` in your `.env` file:

```env
DEFAULT_MODEL=your-model-name
```

### ğŸ“ Example Output

```
Using Python: .venv/bin/python
Python version: Python 3.14.0

Starting evaluation with data: data/test_cases.jsonl
----------------------------------------
Loading data from data/test_cases.jsonl...
Evaluating ID 1...
Evaluating ID 2...
...
------------------------------
Evaluation Finished!
Final Accuracy: 93.33%
```

### ğŸ¤ Contributing

Contributions are welcome! Feel free to:

- Report bugs
- Suggest new features
- Submit pull requests

### ğŸ“„ License

This project is licensed under the MIT License.

---

<a name="chinese"></a>
## ğŸ‡¨ğŸ‡³ ä¸­æ–‡

### ğŸ“– é¡¹ç›®ç®€ä»‹

LLM-Evaluator æ˜¯ä¸€ä¸ªè½»é‡çº§ã€æ¨¡å—åŒ–çš„å¤§è¯­è¨€æ¨¡å‹è¯„æµ‹æ¡†æ¶ï¼Œä¸“ä¸ºå­¦æœ¯ç ”ç©¶å’ŒåŸºå‡†æµ‹è¯•è®¾è®¡ã€‚æ”¯æŒå¤šç§ç­”æ¡ˆæ ¼å¼ï¼ˆLaTeXã€GSM8Kã€è‡ªç„¶è¯­è¨€ï¼‰ï¼Œæä¾›å¯æ‰©å±•çš„è¯„æµ‹ä»»åŠ¡æ¡†æ¶ã€‚

### âœ¨ æ ¸å¿ƒç‰¹æ€§

- ğŸ¯ **å¤šæ ¼å¼ç­”æ¡ˆæå–**ï¼šæ”¯æŒ `\boxed{}`ã€GSM8K æ ¼å¼å’Œè‡ªç„¶è¯­è¨€æ¨¡å¼
- ğŸ”„ **è‡ªåŠ¨é‡è¯•æœºåˆ¶**ï¼šä¼˜é›…å¤„ç† API é™æµå’Œç½‘ç»œé—®é¢˜
- ğŸ“Š **JSONL æ•°æ®é›†æ ¼å¼**ï¼šæ˜“äºåˆ›å»ºå’Œç»´æŠ¤æµ‹è¯•ç”¨ä¾‹
- ğŸŒ **Hugging Face Router æ”¯æŒ**ï¼šå…¼å®¹å¤šç§ LLM æä¾›å•†
- âš™ï¸ **ç¯å¢ƒå˜é‡é…ç½®**ï¼šé€šè¿‡ `.env` å®‰å…¨ç®¡ç† API å¯†é’¥
- ğŸ§ª **å¯å¤ç°ç»“æœ**ï¼šé›¶æ¸©åº¦å‚æ•°ç¡®ä¿è¯„æµ‹ç»“æœä¸€è‡´
- ğŸ“ˆ **äº¤äº’å¼ä»ªè¡¨æ¿**ï¼šåŸºäº Streamlit çš„å¯è§†åŒ–è¯„æµ‹ä¸åˆ†æç•Œé¢

### ğŸš€ å¿«é€Ÿå¼€å§‹

#### 1. å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone <your-repo-url>
cd LLM-Evaluator-1

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# å®‰è£…ä¾èµ–
pip install -r requirements.txt
```

#### 2. é…ç½®

åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `.env` æ–‡ä»¶ï¼š

```env
HF_TOKEN=your_huggingface_token_here
HF_API_BASE=https://router.huggingface.co/v1
DEFAULT_MODEL=deepseek-ai/DeepSeek-V3.2:novita
```

#### 3. å‡†å¤‡æµ‹è¯•æ•°æ®

åœ¨ `data/test_cases.jsonl` ä¸­åˆ›å»ºæµ‹è¯•ç”¨ä¾‹ï¼š

```json
{"id": 1, "question": "If x + 5 = 12, what is x?", "answer": "7"}
{"id": 2, "question": "What is 15 multiplied by 3?", "answer": "45"}
```

#### 4. è¿è¡Œè¯„æµ‹

```bash
# ä½¿ç”¨ Shell è„šæœ¬ï¼ˆæ¨èï¼‰
./run_eval.sh

# æˆ–ç›´æ¥ä½¿ç”¨ Python
python main.py --data_path data/test_cases.jsonl
```

#### 5. å¯åŠ¨äº¤äº’å¼ä»ªè¡¨æ¿

ä½¿ç”¨ Web ç•Œé¢è¿›è¡Œæ›´ç›´è§‚çš„è¯„æµ‹ï¼š

```bash
streamlit run app.py
```

**åŠŸèƒ½ç‰¹æ€§ï¼š**
- ğŸ“ **æ–‡ä»¶ä¸Šä¼ **ï¼šç›´æ¥æ‹–æ‹½ JSONL æ•°æ®é›†
- âš¡ **å¼‚æ­¥è¯„æµ‹**ï¼šæ”¯æŒå¹¶å‘å¤„ç†ä¸è‡ªåŠ¨é™æµ
- ğŸ•¸ï¸ **é›·è¾¾å›¾åˆ†æ**ï¼šå¤šç»´åº¦å±•ç¤ºæ¨¡å‹èƒ½åŠ›
- ğŸ“Š **è¯¦ç»†æŒ‡æ ‡**ï¼šå®æ—¶æŸ¥çœ‹å‡†ç¡®ç‡ä¸å…·ä½“é”™è¯¯ç”¨ä¾‹


### ğŸ“ é¡¹ç›®ç»“æ„

```
LLM-Evaluator-1/
â”œâ”€â”€ main.py                 # ä¸»è¯„æµ‹è„šæœ¬
â”œâ”€â”€ run_eval.sh            # æ‰§è¡ŒåŒ…è£…å™¨ï¼ˆè‡ªåŠ¨æ£€æµ‹ç¯å¢ƒï¼‰
â”œâ”€â”€ requirements.txt       # Python ä¾èµ–
â”œâ”€â”€ .env                   # ç¯å¢ƒé…ç½®ï¼ˆéœ€è‡ªè¡Œåˆ›å»ºï¼‰
â”œâ”€â”€ data/
â”‚   â””â”€â”€ test_cases.jsonl  # æµ‹è¯•æ•°æ®é›†
â””â”€â”€ src/
    â”œâ”€â”€ model_client.py   # LLM API å®¢æˆ·ç«¯
    â””â”€â”€ scorer.py         # ç­”æ¡ˆæå–ä¸è¯„åˆ†é€»è¾‘
```

### ğŸ”§ é«˜çº§ç”¨æ³•

#### è‡ªå®šä¹‰æ•°æ®è·¯å¾„

```bash
./run_eval.sh path/to/custom_data.jsonl
```

#### ç¼–ç¨‹å¼è°ƒç”¨

```python
from src.model_client import LLMClient
from src.scorer import exact_match_scorer

client = LLMClient()
response = client.get_response("What is 2 + 2?")
is_correct = exact_match_scorer(response, "4")
```

### ğŸ“Š æ”¯æŒçš„ç­”æ¡ˆæ ¼å¼

è¯„åˆ†å™¨è‡ªåŠ¨ä»ä»¥ä¸‹æ ¼å¼ä¸­æå–ç­”æ¡ˆï¼š

- **LaTeX æ ¼å¼**ï¼š`\boxed{42}`
- **GSM8K æ ¼å¼**ï¼š`#### 42`
- **è‡ªç„¶è¯­è¨€**ï¼š`"The answer is: 42"`
- **ä¸­æ–‡æ ¼å¼**ï¼š`"ç­”æ¡ˆæ˜¯ï¼š42"`
- **çº¯æ•°å­—**ï¼šå›å¤ä¸­çš„æœ€åä¸€ä¸ªæ•°å­—

### ğŸ› ï¸ è‡ªå®šä¹‰æ‰©å±•

#### æ·»åŠ æ–°çš„è¯„åˆ†æ¨¡å¼

ç¼–è¾‘ `src/scorer.py`ï¼Œåœ¨ `patterns` åˆ—è¡¨ä¸­æ·»åŠ è‡ªå®šä¹‰æ¨¡å¼ï¼š

```python
patterns = [
    r'\\boxed\{([^}]+)\}',
    r'ä½ çš„è‡ªå®šä¹‰æ¨¡å¼',
    # ...
]
```

#### ä½¿ç”¨ä¸åŒçš„æ¨¡å‹

æ›´æ–° `.env` æ–‡ä»¶ä¸­çš„ `DEFAULT_MODEL`ï¼š

```env
DEFAULT_MODEL=ä½ çš„æ¨¡å‹åç§°
```

### ğŸ“ ç¤ºä¾‹è¾“å‡º

```
Using Python: .venv/bin/python
Python version: Python 3.14.0

Starting evaluation with data: data/test_cases.jsonl
----------------------------------------
Loading data from data/test_cases.jsonl...
Evaluating ID 1...
Evaluating ID 2...
...
------------------------------
Evaluation Finished!
Final Accuracy: 93.33%
```

### ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®ï¼æ‚¨å¯ä»¥ï¼š

- æŠ¥å‘Š Bug
- æå‡ºæ–°åŠŸèƒ½å»ºè®®
- æäº¤ Pull Request

### ğŸ“„ å¼€æºåè®®

æœ¬é¡¹ç›®é‡‡ç”¨ MIT åè®®å¼€æºã€‚

---

<div align="center">
Made with â¤ï¸ for LLM Research
</div>